
import torch
from transformers import (
    AutoTokenizer, 
    AutoModelForTokenClassification, 
    TrainingArguments, 
    Trainer,
    DataCollatorForTokenClassification
)
from datasets import Dataset
import numpy as np
from seqeval.metrics import classification_report, f1_score
import json
from sklearn.utils.class_weight import compute_class_weight

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
model_name = "DeepPavlov/bert-base-cased-conversational"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
def load_and_prepare_data(train_file, test_file):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ —Ñ–æ—Ä–º–∞—Ç–µ IOB2"""
    
    def read_iob_file(file_path):
        sentences = []
        current_sentence = []
        
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if not line:  # –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ - –∫–æ–Ω–µ—Ü –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
                    if current_sentence:
                        sentences.append(current_sentence)
                        current_sentence = []
                else:
                    parts = line.split('\t')
                    if len(parts) == 2:
                        token, label = parts
                        current_sentence.append((token, label))
        
        return sentences

    # –ß–∏—Ç–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    train_sentences = read_iob_file(train_file)
    test_sentences = read_iob_file(test_file)
    
    print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(train_sentences)} –æ–±—É—á–∞—é—â–∏—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π")
    print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(test_sentences)} —Ç–µ—Å—Ç–æ–≤—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π")
    
    return train_sentences, test_sentences

# –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è Hugging Face
def convert_to_hf_format(sentences, tokenizer):
    """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ —Ñ–æ—Ä–º–∞—Ç, –ø–æ–Ω—è—Ç–Ω—ã–π –±–∏–±–ª–∏–æ—Ç–µ–∫–µ transformers"""
    
    texts = []
    labels = []
    
    # –ú–∞–ø–ø–∏–Ω–≥ –º–µ—Ç–æ–∫ –≤ —á–∏—Å–ª–æ–≤–æ–π —Ñ–æ—Ä–º–∞—Ç
    label_map = {"O": 0, "B-PRODUCT": 1, "I-PRODUCT": 2}
    
    for sentence in sentences:
        tokens = [item[0] for item in sentence]
        text_labels = [label_map[item[1]] for item in sentence]
        
        # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º —Å —É—á–µ—Ç–æ–º –ø–æ–¥—Ç–æ–∫–µ–Ω–æ–≤
        tokenized_input = tokenizer(
            tokens,
            is_split_into_words=True,
            padding=False,
            truncation=True,
            max_length=256  # –£–º–µ–Ω—å—à–∏–ª–∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É
        )
        
        # –í—ã—Ä–∞–≤–Ω–∏–≤–∞–µ–º –º–µ—Ç–∫–∏ –¥–ª—è –ø–æ–¥—Ç–æ–∫–µ–Ω–æ–≤
        word_ids = tokenized_input.word_ids()
        previous_word_idx = None
        label_ids = []
        
        for word_idx in word_ids:
            if word_idx is None:
                label_ids.append(-100)  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
            elif word_idx != previous_word_idx:
                label_ids.append(text_labels[word_idx])
            else:
                label_ids.append(-100)  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –ø–æ–¥—Ç–æ–∫–µ–Ω—ã
            previous_word_idx = word_idx
        
        texts.append(tokenized_input)
        labels.append(label_ids)
    
    return texts, labels

# –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏
def compute_metrics(p):
    """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞"""
    predictions, labels = p
    predictions = np.argmax(predictions, axis=2)
    
    # –£–±–∏—Ä–∞–µ–º –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º—ã–µ –∏–Ω–¥–µ–∫—Å—ã
    true_predictions = [
        [str(p) for p, l in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]
    true_labels = [
        [str(l) for p, l in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]
    
    # –û–±—Ä–∞—Ç–Ω—ã–π –º–∞–ø–ø–∏–Ω–≥ —á–∏—Å–ª–æ–≤—ã—Ö –º–µ—Ç–æ–∫ –≤ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ
    label_map = {0: "O", 1: "B-PRODUCT", 2: "I-PRODUCT"}
    
    true_predictions_text = [
        [label_map[int(p)] for p in prediction]
        for prediction in true_predictions
    ]
    true_labels_text = [
        [label_map[int(l)] for l in label]
        for label in true_labels
    ]
    
    # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
    try:
        report = classification_report(true_labels_text, true_predictions_text, zero_division=0)
        f1 = f1_score(true_labels_text, true_predictions_text, zero_division=0)
    except:
        report = "Cannot compute metrics"
        f1 = 0
    
    return {
        "f1": f1,
        "report": report
    }

def main():
    print("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ NER –¥–ª—è –º–∞–ª–µ–Ω—å–∫–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞...")
    
    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    print("üìä –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    train_sentences, test_sentences = load_and_prepare_data("train.iob", "test.iob")
    
    if not train_sentences or not test_sentences:
        print("‚ùå –û—à–∏–±–∫–∞: –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
        return
    
    # 2. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    print("üîß –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    train_texts, train_labels = convert_to_hf_format(train_sentences, tokenizer)
    test_texts, test_labels = convert_to_hf_format(test_sentences, tokenizer)
    
    # 3. –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
    train_dataset = Dataset.from_dict({
        "input_ids": [x["input_ids"] for x in train_texts],
        "attention_mask": [x["attention_mask"] for x in train_texts],
        "labels": train_labels
    })
    
    test_dataset = Dataset.from_dict({
        "input_ids": [x["input_ids"] for x in test_texts],
        "attention_mask": [x["attention_mask"] for x in test_texts],
        "labels": test_labels
    })
    
    # 4. –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –±–æ—Ä—å–±—ã —Å –¥–∏—Å–±–∞–ª–∞–Ω—Å–æ–º
    all_labels = []
    for sentence in train_sentences:
        for token, label in sentence:
            label_map = {"O": 0, "B-PRODUCT": 1, "I-PRODUCT": 2}
            all_labels.append(label_map[label])
    
    class_weights = compute_class_weight(
        class_weight='balanced',
        classes=np.unique(all_labels),
        y=all_labels
    )
    class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)
    print(f"–í–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤: {class_weights}")
    
    # 5. –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
    print("ü§ñ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏...")
    model = AutoModelForTokenClassification.from_pretrained(
        model_name,
        num_labels=3,  # O, B-PRODUCT, I-PRODUCT
        id2label={0: "O", 1: "B-PRODUCT", 2: "I-PRODUCT"},
        label2id={"O": 0, "B-PRODUCT": 1, "I-PRODUCT": 2}
    )
    model.to(device)
    
    # 6. –ö–∞—Å—Ç–æ–º–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å —Å —É—á–µ—Ç–æ–º –≤–µ—Å–æ–≤ –∫–ª–∞—Å—Å–æ–≤
    class WeightedLossTrainer(Trainer):
        def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
            # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º num_items_in_batch –∏ –¥—Ä—É–≥–∏–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            labels = inputs.get("labels")
            outputs = model(**inputs)
            logits = outputs.get("logits")
            loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights)
            loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))
            return (loss, outputs) if return_outputs else loss
    
    # 7. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–±—É—á–µ–Ω–∏—è
    training_args = TrainingArguments(
        output_dir="./ner_model_small",
        learning_rate=1e-5, 
        per_device_train_batch_size=2,
        per_device_eval_batch_size=2,
        num_train_epochs=30,  
        weight_decay=0.01,
        eval_strategy="epoch",
        save_strategy="epoch",
        load_best_model_at_end=True,
        metric_for_best_model="f1",
        logging_dir="./logs",
        logging_steps=5,
        report_to="none",
        no_cuda=False if torch.cuda.is_available() else True
    )
    
    data_collator = DataCollatorForTokenClassification(tokenizer)
    
    # 8. –°–æ–∑–¥–∞–Ω–∏–µ —Ç—Ä–µ–Ω–µ—Ä–∞
    trainer = WeightedLossTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=test_dataset,
        data_collator=data_collator,
        tokenizer=tokenizer,
        compute_metrics=compute_metrics
    )
    
    # 9. –û–±—É—á–µ–Ω–∏–µ
    print("üéØ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...")
    train_result = trainer.train()
    
    # 10. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    trainer.save_model()
    tokenizer.save_pretrained("./ner_model_small")
    
    # 11. –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
    print("üìà –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏...")
    eval_results = trainer.evaluate()
    
    print("\n" + "="*50)
    print("–†–ï–ó–£–õ–¨–¢–ê–¢–´ –û–ë–£–ß–ï–ù–ò–Ø:")
    print(f"Final F1 Score: {eval_results['eval_f1']:.3f}")
    print("\nClassification Report:")
    print(eval_results['eval_report'])
    print("="*50)
    
    # 12. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
    with open("training_results_small.json", "w", encoding="utf-8") as f:
        json.dump({
            "f1_score": eval_results["eval_f1"],
            "report": eval_results["eval_report"],
            "train_loss": train_result.training_loss
        }, f, indent=2, ensure_ascii=False)
    
    print("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ! –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ –ø–∞–ø–∫–µ 'ner_model_small'")

if __name__ == "__main__":
    main()